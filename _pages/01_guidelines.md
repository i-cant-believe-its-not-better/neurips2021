---
layout: page
permalink: /guidelines/
title: Reviewer Guidelines
description: ICBINB@NeurIPS 2021
---

# Important Dates
- September 26, 2021: Reviewing period begins
- October 10, 2021: Reviewing period ends

# General Guidelines

One of the main goals of the workshop is to provide an *inclusive venue* for researchers to present their work and get feedback from other researchers in the field. The spirit of the workshop is to evaluate works based on (1) their topical fit with the workshop, (2) the rigor/quality of the research process as opposed to the final performance of the presented approach, and (3) their likelihood of getting constructive feedback that helps the authors.

We have asked authors to categorise their paper as belonging to any of four categories:

1. **Unexpected negative results or anomalies**: ideas that do not provide expected results, yet authors are able to explain why, bringing an interesting closed-form piece of knowledge to the community

2. **Papers that are “stuck” yet contain beautiful/elegant ideas**: Authors should argue why the idea is of interest, rigorously describe the analysis, and include a self-critique

3. **Criticisms of and alternatives to current evaluation metrics and default practices**

4. **Meta-research on the role of “beauty” or negative results in ML research**

# Reviewing criteria:

1. **Summary and contributions**: Briefly summarize the paper and its contributions.

2. **Scope and relevance**:
  - *Fitness*: Do the main ideas of the paper broadly fall under the scope of the workshop (i.e., one of the 4 categories stated above)?
  - *Benefits of inclusion in the workshop*: Will the paper likely benefit from being presented at the workshop, and will the workshop benefit from reading this paper? For papers that contain negative results or criticism of default practices, how surprising do you think the audience of the workshop will find this result? For papers that are “stuck”, does it clearly explain how it is “stuck”? Is it likely that one or more expert attendees can provide tips or feedback to the authors that will get the paper to “work”?

3. **Quality, clarity, and rigor**:
  - *Clarity and reproducibility*: Is the paper well written? Does it describe the main ideas clearly? Are there enough details to reproduce the major results of this work?
  - *Quality/Rigor*: Is the empirical methodology correct? Are the claims in the paper convincing? Is the analysis thorough (e.g., via ablations of the method)? Remember that this workshop is not about “benchmark chasing” and we actively encourage ideas that don’t work! However, we expect even negative results to be supported with solid scientific argumentation and/or experimentation.

4. **Additional feedback:** Constructive comments/suggestions for improvement and questions for the authors.

5. **Overall score**:
  - *Strong accept* [oral presentation]: very likely to benefit from being presented at the workshop, to benefit the community, and to un-stuck/open new interesting research directions
  - *Accept* [spotlight]: likely to benefit from being presented AND likely to benefit the community
  - *Weak accept* [poster]: likely to benefit from being presented OR likely to benefit the community
  - *Weak reject*: suffers from serious clarity issues, unlikely to benefit from being presented AND unlikely to benefit the community
  - *Strong reject*: clearly out of scope or incorrect

6. **Confidence**:
  - Absolutely certain about my score
  - Confident but not absolutely certain
  - Fairly confident
  - Willing to defend the paper, but likely that the reviewer did not understand central parts
  - Educated guess

7. **Expertise**:
  - I have published in this field
  - I am familiar with this field
  - This field is different than mine
